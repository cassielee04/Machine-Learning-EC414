{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5: Logistic Regression and Support Vector Machines\n",
    "\n",
    "by Natalia Frumkin and Karanraj Chauhan with help from B. Kulis, R. Manzelli, and A. Tsiligkardis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: SVM Toy Example\n",
    "\n",
    "Given the following two-class data set:\n",
    "\n",
    "**Class -1: **\n",
    "A = (1,1)\n",
    "B = (2,3)\n",
    "\n",
    "**Class +1: **\n",
    "C = (2,5)\n",
    "D = (4,2)\n",
    "\n",
    "<ol type=\"a\">\n",
    "  <li>Plot the data.</li>\n",
    "  <li>Plot the hyperplane described by w = $(3,2)^T, b = -12$</li>\n",
    "  <li>Calculate the $l_2$ distance of data point C from the hyperplane.</li>\n",
    "  <li>Determine if the hyperplane linearly separates the data. Explain.</li>\n",
    "  <li>Calculate the hard margin SVM hyperplane in canonical form.</li>\n",
    "  <li>Which, if any, data points lie on the SVM hyperplane?</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In this problem, we will use a logistic regression model to classify emails as \"spam\" (1) or \"non-spam\" (0). Recall that the hypothesis/decision rule in a logistic regression model is given by</p>\n",
    "\n",
    "$$h_\\theta(x) = \\sigma(\\theta^Tx) \\\\ \\text{where } \\sigma  \\text{ is the sigmoid function}$$\n",
    "\n",
    "<p>Since logistic regression does not have a closed form solution, we will use gradient descent to obtain the parameters $\\theta$. We will use the negative log likelihood loss with L2 regularization as the loss function. Mathematically, the loss function $l(\\theta)$ for a given set of parameters $\\theta$ will be,</p>\n",
    "\n",
    "$$l(\\theta) = NLL(\\theta) + \\frac{\\lambda}{2}||\\theta||^2 \\\\ \\text{where } NLL(\\theta) = -\\sum_{i=1}^{n} y_i\\log(h(x_i)) + (1 - y_i)\\log(1 - h(x_i))$$\n",
    "\n",
    "<p>The good news is, you won't have to worry about these equations for implementing gradient descent (hurray!). However, what you will need is the gradient or the derivative of the loss function. For a given $n$$ x $$d$ matrix $X$ of data, $n$ x $1$ vector of labels (0/1) $y$, and corresponding $n$ x $1$ vector of predictions $\\hat{y}$, the loss function gradient is</p>\n",
    "\n",
    "$$\\nabla l(\\theta) = (\\hat{y} - y)^{T} \\cdot X + \\lambda \\cdot \\theta$$\n",
    "\n",
    "<ol type=\"a\">\n",
    "    <li>Load the dataset file spambase_data.csv using pandas, and then split the dataset into a train set and a test set. Note: train/test ratio of 0.8/0.2 has been known to work, but you are welcome to try other values.</li>\n",
    "    <li>Using the loss gradient equation above, implement gradient descent (use only the train set for this) to find the parameters $\\theta$ of the logistic regression model. Note: $learning$ $rate = 0.00001$, $\\lambda$ = $10$, and $number$ $of$ $steps = 3000$ have been known to give a decent accuracy but you are welcome to try other values, especially for $number$ $of$ $steps$.</li>\n",
    "    <li>Report the correct classification rate (CCR) of the model on train data and test data. The CCR is defined as $$CCR = \\frac{num\\_correct\\_predictions}{num\\_samples}$$</li>   \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0     1     2    3     4     5     6     7     8     9  ...       48  \\\n",
      "1     0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...    0.000   \n",
      "2     0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...    0.000   \n",
      "3     0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...    0.010   \n",
      "4     0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...    0.000   \n",
      "5     0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...    0.000   \n",
      "6     0.00  0.00  0.00  0.0  1.85  0.00  0.00  1.85  0.00  0.00  ...    0.000   \n",
      "7     0.00  0.00  0.00  0.0  1.92  0.00  0.00  0.00  0.00  0.64  ...    0.000   \n",
      "8     0.00  0.00  0.00  0.0  1.88  0.00  0.00  1.88  0.00  0.00  ...    0.000   \n",
      "9     0.15  0.00  0.46  0.0  0.61  0.00  0.30  0.00  0.92  0.76  ...    0.000   \n",
      "10    0.06  0.12  0.77  0.0  0.19  0.32  0.38  0.00  0.06  0.00  ...    0.040   \n",
      "11    0.00  0.00  0.00  0.0  0.00  0.00  0.96  0.00  0.00  1.92  ...    0.000   \n",
      "12    0.00  0.00  0.25  0.0  0.38  0.25  0.25  0.00  0.00  0.00  ...    0.022   \n",
      "13    0.00  0.69  0.34  0.0  0.34  0.00  0.00  0.00  0.00  0.00  ...    0.000   \n",
      "14    0.00  0.00  0.00  0.0  0.90  0.00  0.90  0.00  0.00  0.90  ...    0.000   \n",
      "15    0.00  0.00  1.42  0.0  0.71  0.35  0.00  0.35  0.00  0.71  ...    0.000   \n",
      "16    0.00  0.42  0.42  0.0  1.27  0.00  0.42  0.00  0.00  1.27  ...    0.000   \n",
      "17    0.00  0.00  0.00  0.0  0.94  0.00  0.00  0.00  0.00  0.00  ...    0.000   \n",
      "18    0.00  0.00  0.00  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...    0.000   \n",
      "19    0.00  0.00  0.55  0.0  1.11  0.00  0.18  0.00  0.00  0.00  ...    0.000   \n",
      "20    0.00  0.63  0.00  0.0  1.59  0.31  0.00  0.00  0.31  0.00  ...    0.000   \n",
      "21    0.00  0.00  0.00  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...    0.000   \n",
      "22    0.05  0.07  0.10  0.0  0.76  0.05  0.15  0.02  0.55  0.00  ...    0.042   \n",
      "23    0.00  0.00  0.00  0.0  2.94  0.00  0.00  0.00  0.00  0.00  ...    0.404   \n",
      "24    0.00  0.00  0.00  0.0  1.16  0.00  0.00  0.00  0.00  0.00  ...    0.000   \n",
      "25    0.00  0.00  0.00  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...    0.000   \n",
      "26    0.05  0.07  0.10  0.0  0.76  0.05  0.15  0.02  0.55  0.00  ...    0.042   \n",
      "27    0.00  0.00  0.00  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...    0.000   \n",
      "28    0.00  0.00  0.00  0.0  0.00  0.00  1.66  0.00  0.00  0.00  ...    0.000   \n",
      "29    0.00  0.00  0.00  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...    0.000   \n",
      "30    0.00  0.00  0.00  0.0  0.65  0.00  0.65  0.00  0.00  0.00  ...    0.000   \n",
      "...    ...   ...   ...  ...   ...   ...   ...   ...   ...   ...  ...      ...   \n",
      "4572  0.00  0.00  0.46  0.0  0.23  0.23  0.00  0.00  0.00  0.00  ...    0.000   \n",
      "4573  0.00  0.00  0.00  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...    0.000   \n",
      "4574  0.00  0.00  0.18  0.0  0.18  0.18  0.00  0.00  0.00  0.00  ...    0.033   \n",
      "4575  0.29  0.00  0.29  0.0  0.00  0.00  0.00  0.00  0.00  0.29  ...    0.000   \n",
      "4576  0.00  0.00  0.00  0.0  0.00  0.00  0.00  0.00  0.00  1.38  ...    0.000   \n",
      "4577  0.00  0.00  0.00  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...    0.000   \n",
      "4578  0.00  0.00  1.20  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...    0.000   \n",
      "4579  0.00  0.00  0.40  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...    0.000   \n",
      "4580  0.27  0.05  0.10  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...    0.607   \n",
      "4581  0.00  0.00  0.00  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...    0.000   \n",
      "4582  0.00  0.00  0.00  0.0  0.00  0.51  0.00  0.00  0.00  0.00  ...    0.000   \n",
      "4583  0.00  0.00  0.00  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...    0.000   \n",
      "4584  0.00  0.00  1.23  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...    0.000   \n",
      "4585  0.00  0.00  0.45  0.0  0.00  0.22  0.00  0.00  0.00  0.00  ...    0.000   \n",
      "4586  0.00  0.00  0.00  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...    0.000   \n",
      "4587  0.00  0.00  0.00  0.0  0.36  0.00  0.00  0.00  0.00  0.00  ...    0.000   \n",
      "4588  0.00  0.00  0.00  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...    0.000   \n",
      "4589  0.00  0.00  3.03  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...    0.000   \n",
      "4590  0.00  0.00  0.00  0.0  0.54  0.00  0.00  0.00  0.00  0.00  ...    0.000   \n",
      "4591  0.00  0.00  0.00  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...    0.000   \n",
      "4592  0.00  0.00  0.00  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...    0.000   \n",
      "4593  0.00  0.00  1.25  0.0  2.50  0.00  0.00  0.00  0.00  0.00  ...    0.000   \n",
      "4594  0.00  0.00  0.00  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...    0.000   \n",
      "4595  0.00  0.00  0.00  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...    0.000   \n",
      "4596  0.00  0.00  1.19  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...    0.000   \n",
      "4597  0.31  0.00  0.62  0.0  0.00  0.31  0.00  0.00  0.00  0.00  ...    0.000   \n",
      "4598  0.00  0.00  0.00  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...    0.000   \n",
      "4599  0.30  0.00  0.30  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...    0.102   \n",
      "4600  0.96  0.00  0.00  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...    0.000   \n",
      "4601  0.00  0.00  0.65  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...    0.000   \n",
      "\n",
      "         49     50     51     52     53      54   55    56  class  \n",
      "1     0.000  0.000  0.778  0.000  0.000   3.756   61   278      1  \n",
      "2     0.132  0.000  0.372  0.180  0.048   5.114  101  1028      1  \n",
      "3     0.143  0.000  0.276  0.184  0.010   9.821  485  2259      1  \n",
      "4     0.137  0.000  0.137  0.000  0.000   3.537   40   191      1  \n",
      "5     0.135  0.000  0.135  0.000  0.000   3.537   40   191      1  \n",
      "6     0.223  0.000  0.000  0.000  0.000   3.000   15    54      1  \n",
      "7     0.054  0.000  0.164  0.054  0.000   1.671    4   112      1  \n",
      "8     0.206  0.000  0.000  0.000  0.000   2.450   11    49      1  \n",
      "9     0.271  0.000  0.181  0.203  0.022   9.744  445  1257      1  \n",
      "10    0.030  0.000  0.244  0.081  0.000   1.729   43   749      1  \n",
      "11    0.000  0.000  0.462  0.000  0.000   1.312    6    21      1  \n",
      "12    0.044  0.000  0.663  0.000  0.000   1.243   11   184      1  \n",
      "13    0.056  0.000  0.786  0.000  0.000   3.728   61   261      1  \n",
      "14    0.000  0.000  0.000  0.000  0.000   2.083    7    25      1  \n",
      "15    0.102  0.000  0.357  0.000  0.000   1.971   24   205      1  \n",
      "16    0.063  0.000  0.572  0.063  0.000   5.659   55   249      1  \n",
      "17    0.000  0.000  0.428  0.000  0.000   4.652   31   107      1  \n",
      "18    0.000  0.000  1.975  0.370  0.000  35.461   95   461      1  \n",
      "19    0.182  0.000  0.455  0.000  0.000   1.320    4    70      1  \n",
      "20    0.275  0.000  0.055  0.496  0.000   3.509   91   186      1  \n",
      "21    0.729  0.000  0.729  0.000  0.000   3.833    9    23      1  \n",
      "22    0.101  0.016  0.250  0.046  0.059   2.569   66  2259      1  \n",
      "23    0.404  0.000  0.809  0.000  0.000   4.857   12    34      1  \n",
      "24    0.133  0.000  0.667  0.000  0.000   1.131    5    69      1  \n",
      "25    0.196  0.000  0.392  0.196  0.000   5.466   22    82      1  \n",
      "26    0.101  0.016  0.250  0.046  0.059   2.565   66  2258      1  \n",
      "27    0.196  0.000  0.392  0.196  0.000   5.466   22    82      1  \n",
      "28    0.000  0.000  0.368  0.000  0.000   2.611   12    47      1  \n",
      "29    0.352  0.000  0.352  0.000  0.000   4.000   11    36      1  \n",
      "30    0.459  0.000  0.091  0.000  0.000   2.687   66   129      1  \n",
      "...     ...    ...    ...    ...    ...     ...  ...   ...    ...  \n",
      "4572  0.082  0.000  0.082  0.000  0.000   1.256    5    98      0  \n",
      "4573  0.254  0.000  0.000  0.000  0.000   1.000    1    13      0  \n",
      "4574  0.033  0.000  0.099  0.000  0.000   1.489   11   137      0  \n",
      "4575  0.107  0.000  0.000  0.000  0.000   1.220    6    61      0  \n",
      "4576  0.213  0.000  0.000  0.000  0.000   1.720   11    43      0  \n",
      "4577  0.131  0.000  0.000  0.000  0.000   1.488    5    64      0  \n",
      "4578  0.000  0.000  0.000  0.000  0.000   1.200    3    24      0  \n",
      "4579  0.000  0.145  0.000  0.000  0.000   1.372    5    70      0  \n",
      "4580  0.064  0.036  0.055  0.000  0.202   3.766   43  1789      0  \n",
      "4581  0.000  0.000  0.000  0.000  0.000   1.571    5    11      0  \n",
      "4582  0.091  0.000  0.091  0.000  0.000   1.586    4    46      0  \n",
      "4583  0.000  0.000  0.000  0.000  0.000   1.266    3    19      0  \n",
      "4584  0.000  0.406  0.000  0.000  0.000   1.666   13    70      0  \n",
      "4585  0.082  0.000  0.041  0.000  0.000   1.500    7   123      0  \n",
      "4586  0.625  0.000  0.000  0.000  0.000   1.375    4    11      0  \n",
      "4587  0.112  0.000  0.000  0.000  0.056   1.793   21   174      0  \n",
      "4588  0.125  0.000  0.000  0.125  0.000   1.272    4    28      0  \n",
      "4589  0.000  0.000  0.000  0.000  0.000   1.111    2    10      0  \n",
      "4590  0.000  0.000  0.000  0.000  0.000   1.000    1    22      0  \n",
      "4591  0.185  0.000  0.000  0.000  0.092   2.468   11    79      0  \n",
      "4592  0.000  0.000  0.000  0.000  0.000   1.000    1     8      0  \n",
      "4593  0.111  0.000  0.000  0.000  0.000   1.285    4    27      0  \n",
      "4594  0.000  0.000  1.052  0.000  0.000   1.000    1     6      0  \n",
      "4595  0.630  0.000  0.000  0.000  0.000   1.727    5    19      0  \n",
      "4596  0.000  0.000  0.000  0.000  0.000   1.000    1    24      0  \n",
      "4597  0.232  0.000  0.000  0.000  0.000   1.142    3    88      0  \n",
      "4598  0.000  0.000  0.353  0.000  0.000   1.555    4    14      0  \n",
      "4599  0.718  0.000  0.000  0.000  0.000   1.404    6   118      0  \n",
      "4600  0.057  0.000  0.000  0.000  0.000   1.147    5    78      0  \n",
      "4601  0.000  0.000  0.125  0.000  0.000   1.250    5    40      0  \n",
      "\n",
      "[4601 rows x 58 columns]\n",
      "(3680, 57)\n",
      "(3680, 58)\n"
     ]
    }
   ],
   "source": [
    "# read in raw dataset\n",
    "spam_data = pd.read_csv(\"spambase_data.csv\", names = ['0','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','21','22','23','24','25','26','27','28','29','30','31','32','33','34','35','36','37','38','39','40','41','42','43','44','45','46','47','48','49','50','51','52','53','54','55','56', \"class\"])\n",
    "spam_data = spam_data.drop(spam_data.index[0])\n",
    "#print(spam_data)\n",
    "X = spam_data\n",
    "print(X)\n",
    "y = spam_data.pop(\"class\")\n",
    "\n",
    "\n",
    "# split into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2)\n",
    "\n",
    "X_train = X_train.values\n",
    "y_train = y_train.values\n",
    "X_test = X_test.values\n",
    "y_test = y_test.values\n",
    "\n",
    "\n",
    "#Augmenting matrix to include bias\n",
    "X_aug = np.append(X_train, np.ones((3680, 1)), axis=1)\n",
    "print(X_train.shape)\n",
    "print(X_aug.shape)\n",
    "\n",
    "#X_test = np.append(X_test, np.ones((921, 1)), axis=1)\n",
    "#print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -1.06158359]\n",
      " [ -1.4015229 ]\n",
      " [ -1.98186525]\n",
      " [  2.97787146]\n",
      " [  5.45785365]\n",
      " [  2.55500971]\n",
      " [  7.37384855]\n",
      " [  3.78161732]\n",
      " [  1.26193381]\n",
      " [ -0.87281769]\n",
      " [  1.53454763]\n",
      " [-15.42473588]\n",
      " [  0.32987073]\n",
      " [  0.3706801 ]\n",
      " [  2.65777827]\n",
      " [ 10.17900939]\n",
      " [  4.29020829]\n",
      " [  3.62483589]\n",
      " [ -1.98189791]\n",
      " [  4.86559334]\n",
      " [  6.79230348]\n",
      " [  4.70437983]\n",
      " [  8.06524566]\n",
      " [  4.95322529]\n",
      " [-42.03604983]\n",
      " [-20.98169343]\n",
      " [-15.54861157]\n",
      " [ -9.08665792]\n",
      " [ -5.5964586 ]\n",
      " [ -6.88269002]\n",
      " [ -3.90514555]\n",
      " [ -2.77744746]\n",
      " [-11.10323257]\n",
      " [ -2.76403333]\n",
      " [ -8.55300376]\n",
      " [ -5.54331524]\n",
      " [-13.63348048]\n",
      " [ -0.84800727]\n",
      " [ -7.40524999]\n",
      " [ -2.50469461]\n",
      " [ -4.86297118]\n",
      " [ -9.06589481]\n",
      " [ -4.63252049]\n",
      " [ -6.64823721]\n",
      " [-13.14624914]\n",
      " [-17.2926447 ]\n",
      " [ -0.78197044]\n",
      " [ -3.49335332]\n",
      " [ -4.0681068 ]\n",
      " [ -7.43957408]\n",
      " [ -1.34514805]\n",
      " [  7.67438327]\n",
      " [  5.11833336]\n",
      " [  1.18008226]\n",
      " [ -7.39933125]\n",
      " [  6.06067798]\n",
      " [ -0.2291197 ]\n",
      " [-32.49069278]]\n"
     ]
    }
   ],
   "source": [
    "# fit logistic regression model\n",
    "\n",
    "#Randomly generate starting w values. Last w value is for bias\n",
    "w = np.random.rand(58, 1)\n",
    "\n",
    "\n",
    "steps = 30000\n",
    "rate = .00001\n",
    "lam = 10\n",
    "\n",
    "\n",
    "for i in range(steps):\n",
    "    y_hat = 1 / (1 + np.exp(-(X_aug[:, 0:57]@w[0:57, :] + w[57])))\n",
    "    #y_hat = np.sign(X_aug[:, 0:57]@w[0:57, :] + w[57])\n",
    "    y_train = y_train.reshape(3680, 1)\n",
    "\n",
    "    #print(i)\n",
    "    #print((X_aug.T @ (y_hat - y_train)).shape)\n",
    "    #print((lam*w).shape)\n",
    "    grad = X_aug.T @ (y_hat - y_train) + lam*w\n",
    "    #print(grad)\n",
    "    w = w - rate*grad\n",
    "\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3680, 1)\n",
      "[-32.49069278 -32.49069278 -32.49069278 ... -32.49069278 -32.49069278\n",
      " -32.49069278]\n",
      "0\n",
      "3085\n",
      "0.8383152173913043\n"
     ]
    }
   ],
   "source": [
    "# predict on test data and train data and calculate CCR\n",
    "#y_pred = X_aug[:, 0:57]@w[0:57, :] + w[57]\n",
    "y_pred = 1 / (1 + np.exp(-(X_aug[:, 0:57]@w[0:57, :] + w[57])))\n",
    "print((X_aug[:, 0:57]@w[0:57, :]).shape)\n",
    "print((w[57]*X_aug[:, 57]))\n",
    "\n",
    "count = 0\n",
    "count2 = 0\n",
    "index = 0\n",
    "for i in y_pred:\n",
    "    if i < 0:\n",
    "        count += 1\n",
    "    if i >= 0.5 and y_train[index] > 0:\n",
    "        count2 += 1\n",
    "    elif i < 0.5 and y_train[index] == 0:\n",
    "        count2 += 1\n",
    "    #print(y_train[index])\n",
    "    #print(np.sign(i))\n",
    "    index += 1\n",
    "    \n",
    "print(count)\n",
    "print(count2)\n",
    "print(count2/index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
